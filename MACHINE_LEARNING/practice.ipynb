{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Binary Classification with the Breast Cancer Dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 62   1]\n",
      " [  2 106]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98        63\n",
      "           1       0.99      0.98      0.99       108\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.98      0.98       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features for better convergence\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# Fit logistic regression model\n",
    "clf = LogisticRegression(max_iter=10000, random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Inferences\n",
    "\n",
    "Based on the binary classification results from the breast cancer model, here are the key inferences:\n",
    "\n",
    "1. **Overall High Accuracy:**\n",
    "   - The model achieves an overall accuracy of **98%** on the test set, indicating that nearly all samples are correctly classified.\n",
    "\n",
    "2. **Balanced Performance for Both Classes:**\n",
    "   - **Class 0 (e.g., benign):**  \n",
    "     - True negatives: 62  \n",
    "     - False positives: 1  \n",
    "   - **Class 1 (e.g., malignant):**  \n",
    "     - True positives: 106  \n",
    "     - False negatives: 2  \n",
    "   This balance suggests that the model is highly effective in differentiating between the two classes.\n",
    "\n",
    "3. **High Precision:**\n",
    "   - **Class 0:** Precision = 0.97  \n",
    "     (97% of the predictions for class 0 are correct)\n",
    "   - **Class 1:** Precision = 0.99  \n",
    "     (99% of the predictions for class 1 are correct)\n",
    "   \n",
    "   High precision minimizes the risk of misclassifying a benign case as malignant or vice versa.\n",
    "\n",
    "4. **High Recall (Sensitivity):**\n",
    "   - Both classes show a recall of **0.98**, meaning the model correctly identifies 98% of the actual instances in each class.\n",
    "   - High recall for the malignant class is crucial in a clinical context to avoid missing true cancer cases.\n",
    "\n",
    "5. **Strong F1-Scores:**\n",
    "   - **Class 0:** F1-score = 0.98  \n",
    "   - **Class 1:** F1-score = 0.99  \n",
    "   These scores reflect an excellent balance between precision and recall.\n",
    "\n",
    "6. **Clinical Implications:**\n",
    "   - The model's robust performance (high accuracy, precision, recall, and F1-scores) suggests that it could serve as a reliable diagnostic tool.\n",
    "   - Particularly, the high recall for malignant cases is vital for early detection, reducing the chance of missing true cancer cases.\n",
    "   - The balanced performance across both classes helps build trust in its predictions, which is essential for clinical decision-making.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Multinomial Classification with the Iris Dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic Regression\n",
      "Confusion Matrix:\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit multinomial logistic regression model\n",
    "clf_multi = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000, random_state=42)\n",
    "clf_multi.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_multi = clf_multi.predict(X_test)\n",
    "print(\"Multinomial Logistic Regression\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_multi))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_multi))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression Inferences\n",
    "\n",
    "\n",
    "\n",
    "1. **Perfect Classification Performance:**\n",
    "   - The confusion matrix shows that each class is perfectly classified:\n",
    "     - **Class 0:** 19 true predictions, 0 misclassifications.\n",
    "     - **Class 1:** 13 true predictions, 0 misclassifications.\n",
    "     - **Class 2:** 13 true predictions, 0 misclassifications.\n",
    "   - The overall accuracy is **100%**, which means every sample in the test set was correctly classified.\n",
    "\n",
    "2. **Model Metrics:**\n",
    "   - **Precision, Recall, and F1-Score:** All are **1.00** for each class. This indicates that:\n",
    "     - Every instance predicted to belong to a given class is correct (high precision).\n",
    "     - All actual instances of each class are correctly identified (high recall).\n",
    "     - The harmonic mean of precision and recall (F1-score) is perfect for every class.\n",
    "\n",
    "3. **Dataset and Model Considerations:**\n",
    "   - The results likely stem from a well-known, easily separable dataset (such as the Iris dataset), which is known to yield near-perfect results with many classification algorithms.\n",
    "   - Although the performance is perfect on the given test set (45 samples), this might be due to the simplicity and small size of the dataset. For larger or more complex datasets, further evaluation and cross-validation would be essential to ensure the model's robustness.\n",
    "\n",
    "4. **Practical Implications:**\n",
    "   - The excellent performance suggests that multinomial logistic regression is highly effective for problems where classes are clearly distinct.\n",
    "   - In practical applications, such high accuracy would instill confidence in the model's predictions, assuming the training and test data are representative of real-world conditions.\n",
    "\n",
    "5. **Next Steps for Validation:**\n",
    "   - Consider performing cross-validation or testing on a larger dataset to confirm that the model generalizes well beyond this specific test set.\n",
    "   - Analyze feature importance and decision boundaries if further interpretability is required.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Ordinal Classification with the Wine Quality Dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                quality   Log-Likelihood:                -1072.0\n",
      "Model:                   OrderedModel   AIC:                             2176.\n",
      "Method:            Maximum Likelihood   BIC:                             2256.\n",
      "Date:                Fri, 14 Mar 2025                                         \n",
      "Time:                        08:30:36                                         \n",
      "No. Observations:                1119                                         \n",
      "Df Residuals:                    1103                                         \n",
      "Df Model:                          11                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "fixed acidity            0.2356      0.169      1.392      0.164      -0.096       0.567\n",
      "volatile acidity        -0.6268      0.086     -7.269      0.000      -0.796      -0.458\n",
      "citric acid             -0.2011      0.108     -1.864      0.062      -0.412       0.010\n",
      "residual sugar           0.0947      0.083      1.147      0.251      -0.067       0.257\n",
      "chlorides               -0.2051      0.077     -2.672      0.008      -0.356      -0.055\n",
      "free sulfur dioxide      0.1530      0.088      1.743      0.081      -0.019       0.325\n",
      "total sulfur dioxide    -0.3651      0.094     -3.878      0.000      -0.550      -0.181\n",
      "density                 -0.1410      0.153     -0.919      0.358      -0.442       0.160\n",
      "pH                      -0.0697      0.110     -0.633      0.526      -0.286       0.146\n",
      "sulphates                0.4351      0.073      5.946      0.000       0.292       0.579\n",
      "alcohol                  0.9371      0.108      8.661      0.000       0.725       1.149\n",
      "3/4                     -5.7308      0.345    -16.616      0.000      -6.407      -5.055\n",
      "4/5                      0.5184      0.181      2.866      0.004       0.164       0.873\n",
      "5/6                      1.3276      0.046     28.574      0.000       1.237       1.419\n",
      "6/7                      1.0448      0.045     22.983      0.000       0.956       1.134\n",
      "7/8                      1.1195      0.096     11.662      0.000       0.931       1.308\n",
      "========================================================================================\n",
      "\n",
      "Ordinal Logistic Regression Confusion Matrix:\n",
      "Predicted    5    6   7\n",
      "Actual                 \n",
      "3            1    0   0\n",
      "4           12    5   0\n",
      "5          147   48   0\n",
      "6           72  116  12\n",
      "7            2   47  12\n",
      "8            0    2   4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the red wine quality dataset\n",
    "wine = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "\n",
    "# Define target (ordinal quality score) and predictors\n",
    "y = wine[\"quality\"]\n",
    "X = wine.drop(\"quality\", axis=1)\n",
    "\n",
    "# Scale features for better convergence\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Fit an ordinal logistic regression (proportional odds) model using statsmodels\n",
    "# Here we use the cumulative logit (\"logit\") model.\n",
    "model = OrderedModel(y_train, X_train, distr=\"logit\")\n",
    "res = model.fit(method='bfgs', disp=False)\n",
    "print(res.summary())\n",
    "\n",
    "# Get predicted probabilities on the test set\n",
    "predicted_probs = res.model.predict(res.params, exog=X_test)\n",
    "# predicted_probs is a NumPy array with shape (n_samples, n_categories)\n",
    "\n",
    "# Define the sorted list of quality levels from the training set\n",
    "quality_levels = sorted(y_train.unique())\n",
    "# For example, quality_levels might be [3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# Use np.argmax to find the index of the highest probability for each observation\n",
    "predicted_index = np.argmax(predicted_probs, axis=1)\n",
    "\n",
    "# Map these indices to the actual quality labels\n",
    "predicted_class = [quality_levels[i] for i in predicted_index]\n",
    "\n",
    "# Evaluate the model: Create a confusion matrix comparing actual vs. predicted quality levels\n",
    "conf_matrix = pd.crosstab(y_test, predicted_class, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "print(\"\\nOrdinal Logistic Regression Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal Logistic Regression Inference\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "- **Objective:**  \n",
    "  The model predicts the ordinal outcome “quality” of wine using several chemical predictors, such as fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol.\n",
    "\n",
    "- **Fit Statistics:**  \n",
    "  - **Log-Likelihood:** -1072.0  \n",
    "  - **AIC:** 2176  \n",
    "  - **BIC:** 2256  \n",
    "  These statistics provide insight into the model’s goodness-of-fit and can be used to compare with other models.\n",
    "\n",
    "## Coefficient Interpretations\n",
    "\n",
    "Each coefficient reflects the change in the log odds of a wine being in a higher quality category for a one-unit increase in the predictor, holding all else constant.\n",
    "\n",
    "- **Volatile Acidity (coef = -0.6268, p < 0.001):**  \n",
    "  A one-unit increase in volatile acidity decreases the odds of being in a higher quality category by a factor of approximately exp(-0.6268) ≈ 0.534. This is statistically significant.\n",
    "\n",
    "- **Alcohol (coef = 0.9371, p < 0.001):**  \n",
    "  Each one-unit increase in alcohol content increases the odds of a higher quality rating by a factor of exp(0.9371) ≈ 2.553, indicating a strong positive effect.\n",
    "\n",
    "- **Sulphates (coef = 0.4351, p < 0.001):**  \n",
    "  Higher sulphate levels are significantly associated with increased wine quality, with an odds ratio of exp(0.4351) ≈ 1.545.\n",
    "\n",
    "- **Total Sulfur Dioxide (coef = -0.3651, p < 0.001):**  \n",
    "  Increasing total sulfur dioxide by one unit reduces the odds of a higher quality rating by exp(-0.3651) ≈ 0.694.\n",
    "\n",
    "- **Other Variables:**  \n",
    "  - *Fixed Acidity (0.2356, p = 0.164), Citric Acid (-0.2011, p = 0.062), Residual Sugar (0.0947, p = 0.251), Density (-0.1410, p = 0.358), and pH (-0.0697, p = 0.526)* did not achieve conventional significance (p < 0.05).  \n",
    "  - Citric acid is marginal (p ≈ 0.062) and might warrant further investigation with additional data or refined modeling.\n",
    "\n",
    "## Thresholds (Cutpoints)\n",
    "\n",
    "The model estimates several thresholds (3/4, 4/5, 5/6, 6/7, 7/8) which serve as boundaries on the latent variable separating the ordinal categories. These cutpoints help in distinguishing between the different quality ratings.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The confusion matrix compares the predicted quality classes (5, 6, and 7) with the actual quality classes (ranging from 3 to 8):\n",
    "\n",
    "| Actual \\ Predicted | 5   | 6   | 7   |\n",
    "|--------------------|-----|-----|-----|\n",
    "| **3**              | 1   | 0   | 0   |\n",
    "| **4**              | 12  | 5   | 0   |\n",
    "| **5**              | 147 | 48  | 0   |\n",
    "| **6**              | 72  | 116 | 12  |\n",
    "| **7**              | 2   | 47  | 12  |\n",
    "| **8**              | 0   | 2   | 4   |\n",
    "\n",
    "- **Interpretation:**  \n",
    "  The majority of predictions are concentrated in the middle categories (5, 6, and 7). However, the model shows some misclassifications, particularly at the extreme ends (actual classes 3, 4, and 8). This suggests that while the model performs well for moderate quality wines, its predictive power diminishes for wines at the very low or high quality extremes.\n",
    "\n",
    "## Overall Inference\n",
    "\n",
    "- **Key Predictors:**  \n",
    "  Significant determinants of wine quality include:\n",
    "  - **Volatile Acidity:** Negative impact (higher acidity decreases quality).\n",
    "  - **Alcohol:** Positive impact (higher alcohol content increases quality).\n",
    "  - **Sulphates:** Positive impact (increases likelihood of higher quality).\n",
    "  - **Total Sulfur Dioxide:** Negative impact (higher levels decrease quality).\n",
    "\n",
    "- **Non-significant Variables:**  \n",
    "  Fixed acidity, residual sugar, density, and pH did not show statistically significant effects. Citric acid’s borderline significance suggests it may have a minor influence that requires further examination.\n",
    "\n",
    "- **Model Performance:**  \n",
    "  The model fits the moderate quality ratings (5, 6, 7) better than the extreme ratings (3, 4, 8). This indicates that while the selected predictors are effective for the central range of wine quality, additional predictors or model adjustments might be needed to accurately capture the extremes.\n",
    "\n",
    "In summary, the ordinal logistic regression analysis indicates that chemical properties such as volatile acidity, alcohol, sulphates, and total sulfur dioxide are key determinants of wine quality. The model effectively discriminates between middle quality levels but may benefit from refinement to improve predictions at the very low and high ends of the quality spectrum.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
