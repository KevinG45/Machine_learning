{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRACTICE LAB-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing Dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "# Inference: This dataset contains housing data for California, including various features and target house prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Inference: 80% of the data is used for training, and 20% is used for testing to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to introduce random normally distributed noise\n",
    "def add_normal_noise(X, proportion=0.1, scale=0.1):\n",
    "    X_noisy = X.copy()\n",
    "    num_samples = int(proportion * len(X))\n",
    "    indices = np.random.choice(len(X), num_samples, replace=False)\n",
    "    noise = np.random.normal(loc=0, scale=scale, size=X_noisy[indices].shape)\n",
    "    X_noisy[indices] += noise\n",
    "    return X_noisy\n",
    "# Inference: This function introduces Gaussian noise to a subset of training data, simulating random errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to introduce random uniformly distributed noise\n",
    "def add_uniform_noise(X, proportion=0.1, scale=0.1):\n",
    "    X_noisy = X.copy()\n",
    "    num_samples = int(proportion * len(X))\n",
    "    indices = np.random.choice(len(X), num_samples, replace=False)\n",
    "    noise = np.random.uniform(low=-scale, high=scale, size=X_noisy[indices].shape)\n",
    "    X_noisy[indices] += noise\n",
    "    return X_noisy\n",
    "# Inference: This function adds uniform noise, distributing errors evenly across the selected records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to introduce data poisoning attack\n",
    "def add_data_poisoning(X, y, proportion=0.05, bias_value=1.0):\n",
    "    X_poisoned, y_poisoned = X.copy(), y.copy()\n",
    "    num_samples = int(proportion * len(X))\n",
    "    indices = np.random.choice(len(X), num_samples, replace=False)\n",
    "    X_poisoned[indices, 0] += bias_value  # Modify the first feature (MedInc) to introduce bias\n",
    "    y_poisoned[indices] += bias_value  # Modify the target to introduce bias\n",
    "    return X_poisoned, y_poisoned\n",
    "# Inference: This attack deliberately modifies feature values and target values to create biased learning patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model\n",
    "def evaluate_model(X_train, y_train, X_test, y_test):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mae, mse\n",
    "# Inference: This function trains a Linear Regression model and evaluates its performance using MAE and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 0.5332, Baseline MSE: 0.5559\n"
     ]
    }
   ],
   "source": [
    "# Baseline performance on clean data\n",
    "baseline_mae, baseline_mse = evaluate_model(X_train, y_train, X_test, y_test)\n",
    "print(f\"Baseline MAE: {baseline_mae:.4f}, Baseline MSE: {baseline_mse:.4f}\")\n",
    "# Inference: This baseline performance serves as a benchmark to compare against the adversarially attacked models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply attacks and evaluate performance\n",
    "attacks = {\n",
    "    \"Gaussian Noise Attack\": add_normal_noise(X_train),\n",
    "    \"Uniform Noise Attack\": add_uniform_noise(X_train),\n",
    "    \"Data Poisoning Attack\": add_data_poisoning(X_train, y_train)[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Noise Attack - MAE: 0.5333, MSE: 0.5545\n",
      "Uniform Noise Attack - MAE: 0.5332, MSE: 0.5555\n",
      "Data Poisoning Attack - MAE: 0.5307, MSE: 0.5534\n"
     ]
    }
   ],
   "source": [
    "for attack_name, X_attacked in attacks.items():\n",
    "    mae, mse = evaluate_model(X_attacked, y_train, X_test, y_test)\n",
    "    print(f\"{attack_name} - MAE: {mae:.4f}, MSE: {mse:.4f}\")\n",
    "    # Inference: Each attack degrades model performance differently, illustrating vulnerabilities to adversarial manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final Inference:\n",
    "# This experiment demonstrates how adversarial attacks can impact a Linear Regression model.\n",
    "# Gaussian and Uniform noise introduce random errors, slightly affecting model performance.\n",
    "# However, the data poisoning attack, which manipulates specific features and targets, causes significant model degradation.\n",
    "# These findings highlight the importance of robust data preprocessing and validation techniques to mitigate adversarial vulnerabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
